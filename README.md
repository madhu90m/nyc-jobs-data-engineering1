ğŸ“Š NYC Jobs Data Engineering Assessment
ğŸ“Œ Project Overview

This project performs an end-to-end analysis of NYC job postings using PySpark.
The objective is to demonstrate data engineering best practices including:

Data exploration and profiling

Data cleaning and transformation

Feature engineering

KPI computation

Visualization

Modular pipeline design

Test coverage

Deployment strategy proposal

The dataset contains job postings from the official New York City Jobs portal, including internal and external postings.

ğŸ— Project Architecture
nyc-jobs-data-engineering/
â”‚
â”œâ”€â”€ data/                   # Raw dataset (nyc-jobs.csv)
â”œâ”€â”€ notebooks/              # Exploratory analysis & visualizations
â”œâ”€â”€ src/                    # Modular PySpark business logic
â”‚   â”œâ”€â”€ data_processing.py
â”‚   â”œâ”€â”€ feature_engineering.py
â”‚   â”œâ”€â”€ kpi_analysis.py
â”‚   â””â”€â”€ utils.py
â”œâ”€â”€ tests/                  # Unit tests
â”œâ”€â”€ output/                 # Processed dataset (Parquet format)
â”œâ”€â”€ main.py                 # Production pipeline entry point
â”œâ”€â”€ MyDocument.md           # Detailed documentation
â”œâ”€â”€ requirements.txt
â””â”€â”€ docker-compose.yml

ğŸ§° Tech Stack

Python 3.x

PySpark

Pandas

Matplotlib

Seaborn

Pytest

Docker (optional deployment)

ğŸ“‚ Dataset

Source: NYC official job postings
Format: CSV
Contains:

Job metadata

Agency information

Salary ranges

Qualification requirements

Posting dates

Job descriptions

ğŸ” Data Exploration

The dataset was analyzed to identify:

Numerical columns (Salary Range From, Salary Range To)

Categorical columns (Agency, Job Category, Posting Type)

Text fields (Job Description, Minimum Qualifications)

Missing values and inconsistencies

ğŸ›  Data Processing

The following transformations were applied:

Null handling

Salary midpoint calculation

Text normalization

Date conversion

Column pruning based on profiling

Processed dataset is stored in Parquet format for optimized querying.

ğŸ§  Feature Engineering

Implemented feature engineering techniques:

Salary midpoint (avg_salary)

Degree requirement binary flag

Experience extraction using regex

Skill-based binary encoding (Python, SQL, AWS, Spark)

ğŸ“Š KPIs Computed

Top 10 job categories by number of postings

Salary distribution per job category

Correlation between degree requirement and salary

Highest salary per agency

Average salary per agency (last 2 years)

Highest paid skills in the US market

Visualizations were generated using Seaborn and Matplotlib.

ğŸš€ How to Run
1ï¸âƒ£ Install Dependencies
pip install -r requirements.txt
2ï¸âƒ£ Run Using Notebook (Recommended for Analysis)
jupyter notebook
Open:
notebooks/assessment_notebook.ipynb
3ï¸âƒ£ Run Production Pipeline
spark-submit main.py
ğŸ“ˆ Spark UI Monitoring

While running:
http://localhost:4040
If unavailable, check:
http://localhost:4041
ğŸ§ª Running Tests
pytest tests/
ğŸ³ Docker Deployment (Optional)

To run using Docker:
docker-compose up --build
This sets up Spark master and worker nodes for distributed execution.

âš™ Deployment Strategy

Proposed deployment approaches:

Apache Airflow for orchestration

AWS EMR / Databricks for scalable execution

S3 Data Lake for storage

Dashboard integration (Power BI / Tableau)

ğŸ“Œ Key Learnings

Importance of modular PySpark design

Handling Spark lazy evaluation

Efficient aggregation before visualization

Avoiding large .toPandas() conversions

Managing distributed computation

ğŸ“ Author

Madhu Mitha
Data Engineering Candidate